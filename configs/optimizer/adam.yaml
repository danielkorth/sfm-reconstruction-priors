_target_: src.opt.optimizer.Adam
name: adam
lr: 1e-5

# Stopping criterion parameters
max_iterations: 300 # Maximum number of iterations before stopping
rel_decrease_threshold: 1e-6 # Minimum relative decrease in cost
patience: 10 # Number of iterations with small decrease before stopping
gradient_norm_threshold: 1e-5 # Minimum gradient norm
param_change_threshold: 1e-5 # Minimum change in parameters

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  factor: 0.5
  patience: 0
  verbose: True
